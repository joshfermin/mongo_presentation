<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Mongo Sharding + GridFS - Josh Fermin</title>

		<meta name="description" content="Mongo Sharding + GridFS">
		<meta name="author" content="Josh Fermin">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/solarized.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h3>Mongo Sharding + GridFS</h3>
					<p>
						<small>By <a href="http://joshfermin.me">Josh Fermin</a></small>
					</p>
				</section>

				<section>
					<h2>What is Sharding?</h2>
					<p class="fragment">
						Storing data across multiple nodes/machines.
					</p>
					<p class="fragment">
						A single machine may run out of storage or have bottlenecked reads and writes
					</p>
					<p class="fragment">
						Sharding is horizontally scalable - Add machines as demand increases.
					</p>

					<aside class="notes">
						
					</aside>
				</section>

				<section>
					<section>
						<h2>Database Issues</h2>
						<p class="fragment">
							High throughput and large amounts data put a lot of stress on a single server.
						</p>
						<p class="fragment">
							Set sizes larger than RAM stress I/O capacity
						</p>
						<p class="fragment">
							High query rates can exhuast CPU capacity of a single server.
						</p>
						<aside class="notes">
							Spark was developed in 2009 at UC Berkeley AMPLab, open sourced in 2010, and became a top-level Apache project in February, 2014. It has since become â€¨one of the largest open source communities in big data, with over 200 contributors in 50+ organizations.
						</aside>
					</section>
					<section>
						<h2>Two Solutions</h2>
						<ul>
							<li class="fragment">Vertical Scaling:</li>
								<ul>
									<li class="fragment">Add more CPU and storage resources to a single server</li>
									<li class="fragment">Limitations: Expensive.</li>
								</ul> 

							</br>
							<li class="fragment">Sharding (Horizontal Scaling): </li>
								<ul>
									<li class="fragment">Distribute data set over multiple servers or shards</li>
									<li class="fragment">Each shard is an independent db and all together they make up a single logical db.</li>
								</ul>
						</ul>
						<aside class="notes">
						Vertical scaling adds more CPU and storage resources. High performance systems with large numbers of CPUs and large amounts of RAM are disproportionately more expensive than smaller systems. 
						
						</aside>
					</section>
					<section>
						<h2>Sharded Collection</h2>
						<img src="lib/img/sharded-collection.png"></img>

						<aside class="notes">
							Sharding reduces the number of operations each shard handles. Each shard processes fewer operations as the cluster grows. As a result, a cluster can increase capacity and throughput horizontally.

							For example, to insert data, the application only needs to access the shard responsible for that record.
							Sharding reduces the amount of data that each server needs to store. Each shard stores less data as the cluster grows.
							For example, if a database has a 1 terabyte data set, and there are 4 shards, then each shard might hold only 256GB of data. If there are 40 shards, then each shard might hold only 25GB of data.
						</aside>
					</section>
				</section>

				<section>
					<section>
						<h2>Sharding in MongoDB</h2>
						<img src="lib/img/sharded-cluster-production-architecture.png"></img>
						<aside class="notes">
							example of mongodb architecture, shards store data (replica sets provide high availability and data consistency (master-slave))

							Query routers interface with client apps and give the client what information it needs from ceratin shards

							config servers store cluster's metadata. production sharded clusters need exactly 3 config servers
						</aside>
					</section>
					<section>
						<h2>Data Partitioning</h2>
						<p class="fragment">
							Sharding happens on a collection level.
						</p>
						<p class="fragment">
							Sharding divides a collection's data by a shard key
						</p>
						<p class="fragment">
							Two types of partitioning: <b>range based</b> and <b>hash based</b>
						</p>
						<aside class="notes">
							example of mongodb architecture, shards store data (replica sets provide high availability and data consistency (master-slave))

							Query routers interface with client apps and give the client what information it needs from ceratin shards

							config servers store cluster's metadata. production sharded clusters need exactly 3 config servers
						</aside>
					</section>
					
					<section>
						<h2>Ranged Based Paritioning</h2>
						<img src="lib/img/sharding-range-based.png"></img>
						<aside class="notes">
							Ranged based sharding - MongoDB divides data set into ranges determined by shard key values. I.e. if x is your shardkey all x's between -75 and 25 will be on one shard... etc
						</aside>
					</section>
					<section>
						<h2>Hash Based Paritioning</h2>
						<img src="lib/img/sharding-hash-based.png"></img>
						<aside class="notes">
							Hash based sharding uses a hash function to determine where each piece of data goes to which shard.
						</aside>
					</section>
				</section>

				<section>
					<section>
						<h2>Mongo Sharding Tutorial</h2>
						<p>
						Taken from <a href="http://docs.mongodb.org/manual/tutorial/deploy-shard-cluster/">Mongo Docs.</a>
						</p>
						<aside class="notes">
							
						</aside>
					</section>
					<section>
						<h2>Fire Up Config Server</h2>
						<small>Download Mongo<a href="http://docs.mongodb.org/manual/installation/"> here</a></small>
							<pre><code>mkdir /data/configdb</pre></code>
							<pre><code>mongod --configsvr --dbpath /data/configdb</pre></code>
						<ul>
							<li>data/configdb where server metadata will be stored</li>
							<li>In production, run configsvr on three different hosts</li>
						</ul>
						<aside class="notes">
							Start config server that holds metadata for cluster


							- configsvr is flag to tell mongo that it is a config server
							- data/configdb is where the metadata about the server will be stored - (Default listening on port 27019)
							- usually running configsvr on three different hosts

						</aside>
					</section>

					<section>
					<h2>Setup Mongos - Routing Service</h2>
					<pre><code>mongos --configdb localhost:27019 --port 27017</pre></code>
					<ul>
						<li>Lightweight, do not require data directories</li>
						<li>Usually have two of these in production</li>
					</ul>
					<aside class="notes">
						In a separate terminal window, use this command which will start the mongos router instance which wil route all queries to the database for you to get the correct data
					</aside>
					</section>
					
					<section>
						<h2>Create DBs for Shards</h2>
						<pre><code>mkdir /data/shard1</pre></code>
						<pre><code>mongod --dbpath /data/shard1 --port 27018 --shardsvr</pre></code>
						<ul>
							<li></li>
							<li>Distributed collection of items</li>
							<li>Can be stored in the volatile memory or in a persistent storage </li>
						</ul>
					</section>

					<section>
						<h2>RDDs</h2>
						<img src="http://3.bp.blogspot.com/-J7R0TMXgz04/UuKIuoeANDI/AAAAAAAAKxM/RyuhMwSLLS0/s1600/mr-flow.png"></img>
						<ul>
						<li> Reading and writing happens too often for each Map Reduce (MR)</li>
						</ul>
					</section>

					<section>
						<h2>RDDs</h2>
						<img src="http://2.bp.blogspot.com/-QjW6MrkQ4hE/UuKIgacmG4I/AAAAAAAAKxE/EGXeX7UUDxc/s1600/rdd-flow.png"></img>
						<ul>
						<li> Transformations from one to another are through memory and doesn't touch the disk (except for RDD2)</li>
						<li>When the memory runs out, it is usually moved over to persistent storage.</li>
						</ul>
						<aside class="notes">Note that in the above flow, the RDD2 is persisted to disk because of Check Pointing. In the work flow, for any failure during the transformations t3 or t4, the entire work flow need not be played back because the RDD2 is persisted to disk. It would be enough if transformation t3 and t4 are played back.</aside>
					</section>

					<section data-markdown>
					<script type="text/template">
					<h2>RDD Actions</h2>
					```scala
					scala> textFile.count() // Number of items in this RDD
					res0: Long = 126

					scala> textFile.first() // First item in this RDD
					res1: String = # Apache Spark
					```

					Note: RDDs have actions which return values. 
					</script>
					</section>

					<section>
					<h2>RDD Transformations</h2>
					<pre><code>
val linesWithSpark = textFile.filter(line => line.contains("Spark"))
linesWithSpark: spark.RDD[String] = spark.FilteredRDD@7dd4af09
					</code></pre>
					<aside class="notes">
					RDDs also have transformations which returns a pointer to a new RDDs, in this case it will return a subset of the previous RDD with the lines that only contain the word "Spark"
					</aside>
					</section>

					<section data-markdown>
					<script type="text/template">
						<h2>Chaining Actions and transformations</h2>

						```scala
						// How many lines contain "Spark"?
						textFile.filter(line => line.contains("Spark")).count() 
						res3: Long = 15
						```
					</script>
					</section>
				</section>

				
<section>
				<section data-markdown>
					<script type="text/template">
						## More RDD Operations
						Finding the longest line in a text file:
						```
						textFile.map(line => line.split(" ").size)
							.reduce((a, b) => if (a > b) a else b)
res4: Long = 15
						```

						```
						scala> import java.lang.Math
						import java.lang.Math

						textFile.map(line => line.split(" ").size)
							.reduce((a, b) => Math.max(a, b))
						res5: Int = 15
						```
							
Note: This first maps a line to an integer value, creating a new RDD. reduce is called on that RDD to find the largest line count. The arguments to map and reduce are Scala function literals (closures), and can use any language feature or Scala/Java library.
					</script>
					
				</section>
				<section data-markdown>
					<script type="text/template">
						## Map Reduce
						Finding word count
						```
scala> val wordCounts = textFile.flatMap(line => line.split(" "))
 		.map(word => (word, 1)).reduceByKey((a, b) => a + b)
wordCounts: spark.RDD[(String, Int)] = spark.ShuffledAggregatedRDD@71f027b8
						```

						```
						scala> wordCounts.collect()
res6: Array[(String, Int)] = Array((means,1), (under,2),
 (this,3), (Because,1), (Python,2), (agree,1), (cluster.,1), ...)
```

Note: Here, we combined the flatMap, map and reduceByKey transformations to compute the per-word counts in the file as an RDD of (String, Int) pairs. To collect the word counts in our shell, we can use the collect action
					</script>
					<aside class="notes">
					
					</aside>
				</section>
</section>
	
				

				<section>
					<section>
						<h2>Caching</h2>
						<p class="fragment">Cluster-wide in-memory cache</p>
						<p><span class="fragment">Useful for when data is accessed repeatedly (i.e. iterative algorithms or re-querying a small dataset)</p>

						<aside class="notes">
							The interesting part is that these same functions can be used on very large data sets, even when they are striped across tens or hundreds of nodes. You can also do this interactively by connecting bin/spark-shell to a cluster
						</aside>
					</section>
					<section>
						<h2>Caching Example</h2>
						<pre><code>scala> linesWithSpark.cache()
res7: spark.RDD[String] = spark.FilteredRDD@17e51082

scala> linesWithSpark.count()
res8: Long = 15</code></pre>
					<aside class="notes">
							
					</aside>
					</section>
				</section>

				<section>
				<h2>Where to go from here</h2>
				<ul>
				<li><a href="https://spark.apache.org/docs/latest/programming-guide.html">Spark Programming Gudie</a></li>
				<li><a href="https://spark.apache.org/docs/latest/cluster-overview.html">Deployment overview</a></li>
				<li>Spark Examples</li>
				</ul>
				</section>

				<section>
				<h2>Who uses spark?</h2>
				<ul>
				<li>SK Telecom - Analyze mobile usage patterns</li>
				<li>Freeman Lab - analyzing/visualizing patterns in large scale recordings of brain acitivty</li>
				<li>Yandex - Using spark to process islands identified from  a search robot</li>
				</ul>
				</section>
				<section>
					<h2>Resources</h2>
					<ul>
						<li><a href="https://spark.apache.org/docs/latest/index.html">Spark Quick Start / Spark Docs</a></li>
						<li><a href="http://stanford.edu/~rezab/sparkclass/slides/itas_workshop.pdf">Intro To Apache Spark - Stanford</a></li>
						<li><a href="http://www.thecloudavenue.com/2014/01/resilient-distributed-datasets-rdd.html">Learning about RDDs</a></li>
					</ul>
				</section>
			</div>
		</div>







		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				// Optional reveal.js plugins
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true }
				]
			});

		</script>

	</body>
</html>
